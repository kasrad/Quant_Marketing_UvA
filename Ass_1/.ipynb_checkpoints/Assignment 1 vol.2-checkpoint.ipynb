{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('trainshuf.csv', nrows = 20000)\n",
    "data.columns = ['id', 'click', 'hour', 'C1', 'banner_pos', 'site_id', 'site_domain', 'site_category', \n",
    "'app_id', 'app_domain', 'app_category', 'device_id', 'device_ip', 'device_model', 'device_type', \n",
    "'device_conn_type', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_prep(data):\n",
    "    data.columns = ['id', 'click', 'hour', 'C1', 'banner_pos', 'site_id', 'site_domain',\n",
    "                    'site_category', 'app_id', 'app_domain', 'app_category', 'device_id',\n",
    "                    'device_ip', 'device_model', 'device_type', 'device_conn_type', 'C14',\n",
    "                    'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21']\n",
    "    data = data.drop('id', axis=1)\n",
    "    data = data.drop('C1', axis = 1)\n",
    "    data = data.drop('C14', axis = 1)\n",
    "    str(data['hour'][4])[-2:]\n",
    "    hours = [str(i)[-2:] for i in data['hour']]\n",
    "    data['hours_clean'] = hours\n",
    "    data = data.drop('hour', axis = 1)\n",
    "    for i in range(1,data.shape[1]):\n",
    "        le = LabelEncoder()\n",
    "        le.fit(data.iloc[:, i].unique())\n",
    "        data.iloc[:, i] = le.transform(data.iloc[:, i])\n",
    "    return(data.drop('click', axis = 1), data['click'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = data.drop('id', axis=1)\n",
    "data = data.drop('C1', axis = 1)\n",
    "data = data.drop('C14', axis = 1)\n",
    "str(data['hour'][4])[-2:]\n",
    "hours = [str(i)[-2:] for i in data['hour']]\n",
    "data['hours_clean'] = hours\n",
    "print(data['hour'][:5])\n",
    "print(data['hours_clean'][:5])\n",
    "data = data.drop('hour', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,data.shape[1]):\n",
    "        le = LabelEncoder()\n",
    "        le.fit(data.iloc[:, i].unique())\n",
    "        data.iloc[:, i] = le.transform(data.iloc[:, i])\n",
    "data.drop('click', axis = 1).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# enc = OneHotEncoder(categorical_features='all', dtype='int64',\n",
    "#        handle_unknown='error', n_values='auto', sparse=True)\n",
    "# x = enc.fit_transform(data.drop('click', axis = 1))\n",
    "# y = data['click']\n",
    "# print(x.shape)\n",
    "# print(y.shape)\n",
    "# print(type(x))\n",
    "# print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('trainshuf.csv', nrows = 1000000)\n",
    "x, y = data_prep(data1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20)\n",
      "(1000000, 20)\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('trainshuf.csv', skiprows = 1000000, nrows = 1000)\n",
    "x2, y2 = data_prep(data2)\n",
    "print(x2.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dummyPy import OneHotEncoder\n",
    "encoder = OneHotEncoder(x.columns)\n",
    "encoder.fit(x)\n",
    "# encoder.transform(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-3204f9e85ae5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mencoded_data1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mencoded_data1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Quant_Marketing_UvA\\Ass_1\\dummyPy.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    142\u001b[0m                             \u001b[1;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                             for column_name in data.columns]\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformed_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoded_data1 = encoder.transform(x2)\n",
    "encoded_data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 8096)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data2 = encoder.transform(x2)\n",
    "encoded_data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data.corr()\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pohlidat si deleni na train a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(encoded_data1, y, test_size = 0.2, random_state = 1211)\n",
    "# print(X_train.head())\n",
    "# # sc = StandardScaler()\n",
    "# # X_train = sc.fit_transform(X_train)\n",
    "# # X_test = sc.transform(X_test)\n",
    "# # X_train = pd.DataFrame(X_train)\n",
    "# # X_test = pd.DataFrame(X_test)\n",
    "# print(X_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = SGDClassifier(loss='log', \n",
    "                     penalty='l1', \n",
    "                     alpha=0.0001, \n",
    "                     fit_intercept=False, \n",
    "                     max_iter=100,\n",
    "                     shuffle=False,\n",
    "                     n_jobs=1,\n",
    "                     random_state=19, \n",
    "                     learning_rate='optimal', \n",
    "                     class_weight= None,\n",
    "                     warm_start = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0922052738004044\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "logit.partial_fit(x_train, y_train, classes = np.unique(y))\n",
    "target_predicted = logit.predict_proba(x_test)\n",
    "print(np.mean(target_predicted[:,1]))\n",
    "print(np.sum(target_predicted[:,1] > 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n",
      "480\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(((target_predicted[:,1] > 0.5) + y_test) == 2))\n",
    "print(np.sum(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27417722696826485"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(target_predicted[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.fit(X_train.drop(['id','click'], axis = 1), X_train['click'])\n",
    "x = logit.predict_proba(X_test.drop(['id','click'], axis = 1))\n",
    "print(x.shape)\n",
    "print(np.mean(x))\n",
    "print(np.mean(x[:,1]))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "class Encoder():\n",
    "    \"\"\"\n",
    "    Helper class to encode levels of a categorical Variable.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.column_mapper = None\n",
    "\n",
    "    def fit(self, levels):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        levels: set\n",
    "            Unique levels of the categorical variable.\n",
    "        \"\"\"\n",
    "        self.column_mapper = {x:i for i,x in enumerate(sorted(levels))}\n",
    "\n",
    "    def transform(self, column_data):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        columns_data: pandas Series object\n",
    "        \"\"\"\n",
    "        row_cols = [(i, self.column_mapper[x])\n",
    "                    for i,x in enumerate(column_data) if x in self.column_mapper]\n",
    "        data = np.ones(len(row_cols))\n",
    "\n",
    "        return(coo_matrix((data, zip(*row_cols)),\n",
    "                          shape=(column_data.shape[0], len(self.column_mapper))))\n",
    "\n",
    "\n",
    "class OneHotEncoder():\n",
    "    \"\"\"\n",
    "    A One Hot Encoder class that converts the categorical variables in a data frame\n",
    "    to one hot encoded variables. It can also handle large data that is too big to fit\n",
    "    in the memory by reading the data in chunks.\n",
    "    Example\n",
    "    -------\n",
    "    The following example uses the kaggle's titanic data. It can be found here -\n",
    "    `https://www.kaggle.com/c/titanic/data`\n",
    "    This data is only 60 KB and it has been used for a demonstration purpose.\n",
    "    This class also works well with datasets too large to fit into the machine\n",
    "    memory.\n",
    "    >>> from dummyPy import OneHotEncoder\n",
    "    >>> import pandas as pd\n",
    "    >>> encoder = OneHotEncoder(categorical_columns=[\"Pclass\", \"Sex\", \"Embarked\"])\n",
    "    >>> data = pd.read_csv(\"titanic.csv\", usecols=[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\"])\n",
    "    >>> data.shape\n",
    "    (891, 5)\n",
    "    >>> encoder.fit(data)\n",
    "    >>> X = encoder.transform(data)\n",
    "    >>> X.shape\n",
    "    (891, 11)\n",
    "    >>> X\n",
    "    array([[0.0, 0.0, 1.0, ..., 0.0, 0.0, 1.0],\n",
    "           [1.0, 0.0, 0.0, ..., 1.0, 0.0, 0.0],\n",
    "           [0.0, 0.0, 1.0, ..., 0.0, 0.0, 1.0],\n",
    "           ...,\n",
    "           [0.0, 0.0, 1.0, ..., 0.0, 0.0, 1.0],\n",
    "           [1.0, 0.0, 0.0, ..., 1.0, 0.0, 0.0],\n",
    "           [0.0, 0.0, 1.0, ..., 0.0, 1.0, 0.0]], dtype=object)\n",
    "    >>> chunked_data = pd.read_csv(\"titanic.csv\",\n",
    "                                    usecols=[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\"],\n",
    "                                    chunksize=10)\n",
    "    >>> encoder2 = OneHotEncoder(categorical_columns=[\"Pclass\", \"Sex\", \"Embarked\"])\n",
    "    >>> encoder2.fit(chunked_data)\n",
    "    >>> X = encoder2.transform(data)\n",
    "    >>> X.shape\n",
    "    (891, 11)\n",
    "    >>> X\n",
    "    array([[0.0, 0.0, 1.0, ..., 0.0, 0.0, 1.0],\n",
    "           [1.0, 0.0, 0.0, ..., 1.0, 0.0, 0.0],\n",
    "           [0.0, 0.0, 1.0, ..., 0.0, 0.0, 1.0],\n",
    "           ...,\n",
    "           [0.0, 0.0, 1.0, ..., 0.0, 0.0, 1.0],\n",
    "           [1.0, 0.0, 0.0, ..., 1.0, 0.0, 0.0],\n",
    "           [0.0, 0.0, 1.0, ..., 0.0, 1.0, 0.0]], dtype=object)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, categorical_columns):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        categorical_columns: list\n",
    "            A list of the names of the categorical varibales in the data. All these columns\n",
    "            must have dtype as string.\n",
    "        \"\"\"\n",
    "\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.unique_vals = defaultdict(set)\n",
    "        self.encoders = {column_name: Encoder() for column_name in categorical_columns}\n",
    "\n",
    "    def _update_unique_vals(self, data):\n",
    "        for column_name in self.categorical_columns:\n",
    "            for value in data[column_name]: \n",
    "            \tself.unique_vals[column_name].add(value)\n",
    "\n",
    "    def _fit_encoders(self):\n",
    "        for column_name in self.categorical_columns:\n",
    "            self.encoders[column_name].fit(self.unique_vals[column_name])\n",
    "\n",
    "    def fit(self, data):    \n",
    "        \"\"\"\n",
    "        This method reads the categorical columns and gets the necessary\n",
    "        one hot encoded column shapes.\n",
    "        It can also read the data in chunks.\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: pandas.core.frame.DataFrame or pandas.io.parsers.TextFileReader\n",
    "            The data can be either a pandas data frame or a pandas TextFileReader\n",
    "            object. The TextFileReader object is created by specifying the \n",
    "            chunksize parameter in pandas read_csv method.\n",
    "        \n",
    "            Use the TextFileReader object as input if the dataset is too large to\n",
    "            fit in the machine memory.\n",
    "        \"\"\"\n",
    "        if(isinstance(data, pd.core.frame.DataFrame)):\n",
    "            self._update_unique_vals(data)\n",
    "        else:\n",
    "            for data_chunk in data:\n",
    "                self._update_unique_vals(data_chunk)\n",
    "\n",
    "        self._fit_encoders() \n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        This method is used to convert the categorical values in your data into\n",
    "        one hot encoded vectors. It convets the categorical columns in the data\n",
    "        to one hot encoded columns and leaves the continuous variable columns as it is.\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: pandas data frame\n",
    "            The data frame object that needs to be transformed.\n",
    "        \"\"\"\n",
    "        transformed_data = [self.encoders[column_name].transform(data[column_name]).toarray()\n",
    "                            if column_name in self.categorical_columns\n",
    "                            else data[column_name].values.reshape(-1, 1)\n",
    "                            for column_name in data.columns]\n",
    "        return(np.array(np.concatenate(transformed_data, axis=1), dtype=object))\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        \"\"\"\n",
    "        This method calls fit and transform one after the other.\n",
    "        \n",
    "        Please note that unlike the fit method the fit_transform method\n",
    "        can take only the pandas data frame as input. \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: pandas.core.frame.DataFrame\n",
    "            A pandas data frame.\n",
    "        \"\"\"\n",
    "        self.fit(data)\n",
    "        return(self.transform(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
